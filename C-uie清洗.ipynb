{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16310d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取文件: uie结果完整版.xlsx\n",
      "\n",
      "原始数据行数: 5900\n",
      "\n",
      "=== 检查index和raw_index一致性 ===\n",
      "✓ 所有index和raw_index一致\n",
      "\n",
      "=== 过滤species为空的行 ===\n",
      "删除 748 条species为空的记录\n",
      "过滤后数据行数: 5152\n",
      "\n",
      "=== 处理locations字段 ===\n",
      "⚠️  发现 1 条locations格式异常的记录,将删除:\n",
      "  行 4519: {'extra_addr': None, 'struc_addr': None}\n",
      "处理后数据行数: 5151\n",
      "\n",
      "=== 处理times字段 ===\n",
      "\n",
      "时间处理状态统计:\n",
      "  ✓ 原始times格式正确: 4834 条\n",
      "  ⚠️  从date列提取: 317 条\n",
      "\n",
      "最终数据行数: 5151\n",
      "\n",
      "保存清洗后的数据到: output/uie清洗待整理&人工审核.xlsx\n",
      "\n",
      "============================================================\n",
      "数据清洗摘要\n",
      "============================================================\n",
      "原始记录数: 5900\n",
      "清洗后记录数: 5151\n",
      "删除记录数: 749\n",
      "  - species为空: 748\n",
      "  - locations异常: 1\n",
      "  - times和date都无法提取: 0\n",
      "\n",
      "新增列(保留所有原始列):\n",
      "  - struc_addr: 从locations提取的结构化地址\n",
      "  - times_clean: 清洗后的时间(yyyy-mm格式)\n",
      "  - time_source: 时间来源标记 (ok/from_date)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Excel数据清洗脚本 - Jupyter Notebook版本\n",
    "功能:\n",
    "1. 检查index和raw_index是否一致\n",
    "2. 过滤species为空的行\n",
    "3. 验证并提取locations中的struc_addr\n",
    "4. 处理times字段,为空时从date列提取,异常标记供审核\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_data(input_file, output_file=None):\n",
    "    \"\"\"\n",
    "    清洗Excel数据\n",
    "    \n",
    "    Args:\n",
    "        input_file: 输入xlsx文件路径\n",
    "        output_file: 输出xlsx文件路径(可选,默认为input_cleaned.xlsx)\n",
    "    \n",
    "    Returns:\n",
    "        df_cleaned: 清洗后的DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # 读取Excel文件\n",
    "    print(f\"读取文件: {input_file}\")\n",
    "    df = pd.read_excel(input_file)\n",
    "    \n",
    "    # 验证列名\n",
    "    required_cols = ['index', 'raw_index', 'species', 'locations', 'times', 'date']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"缺少必需的列: {missing_cols}\")\n",
    "    \n",
    "    print(f\"\\n原始数据行数: {len(df)}\")\n",
    "    \n",
    "    # 1. 检查index和raw_index是否一致\n",
    "    print(\"\\n=== 检查index和raw_index一致性 ===\")\n",
    "    mismatch = df[df['index'] != df['raw_index']]\n",
    "    if len(mismatch) > 0:\n",
    "        print(f\"⚠️  发现 {len(mismatch)} 条不一致的记录:\")\n",
    "        for idx, row in mismatch.head(10).iterrows():\n",
    "            print(f\"  行 {idx+2}: index={row['index']}, raw_index={row['raw_index']}\")\n",
    "        if len(mismatch) > 10:\n",
    "            print(f\"  ... 还有 {len(mismatch)-10} 条\")\n",
    "    else:\n",
    "        print(\"✓ 所有index和raw_index一致\")\n",
    "    \n",
    "    # 2. 删除species为空的行\n",
    "    print(\"\\n=== 过滤species为空的行 ===\")\n",
    "    df_cleaned = df.copy()\n",
    "    empty_species = df_cleaned['species'].isna() | (df_cleaned['species'] == '')\n",
    "    empty_count = empty_species.sum()\n",
    "    \n",
    "    if empty_count > 0:\n",
    "        print(f\"删除 {empty_count} 条species为空的记录\")\n",
    "        df_cleaned = df_cleaned[~empty_species].copy()\n",
    "    else:\n",
    "        print(\"✓ 没有species为空的记录\")\n",
    "    \n",
    "    print(f\"过滤后数据行数: {len(df_cleaned)}\")\n",
    "    \n",
    "    # 3. 处理locations字段\n",
    "    print(\"\\n=== 处理locations字段 ===\")\n",
    "    \n",
    "    def extract_struc_addr(loc_str):\n",
    "        \"\"\"\n",
    "        从locations字符串中提取struc_addr\n",
    "        支持两种格式:\n",
    "        1. dict: {'extra_addr': '...', 'struc_addr': '...'}\n",
    "        2. list(dict): [{'extra_addr': '...', 'struc_addr': '...'}, ...]\n",
    "        \"\"\"\n",
    "        if pd.isna(loc_str) or loc_str == '':\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # 尝试将字符串转换为Python对象\n",
    "            loc_obj = ast.literal_eval(str(loc_str))\n",
    "            \n",
    "            # 如果是列表,取第一个元素\n",
    "            if isinstance(loc_obj, list) and len(loc_obj) > 0:\n",
    "                loc_obj = loc_obj[0]\n",
    "            \n",
    "            # 检查是否为字典且包含struc_addr\n",
    "            if isinstance(loc_obj, dict) and 'struc_addr' in loc_obj:\n",
    "                return loc_obj['struc_addr']\n",
    "            else:\n",
    "                return None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    # 提取struc_addr\n",
    "    df_cleaned['struc_addr'] = df_cleaned['locations'].apply(extract_struc_addr)\n",
    "    \n",
    "    # 统计异常的locations\n",
    "    valid_locations = df_cleaned['struc_addr'].notna()\n",
    "    invalid_count = (~valid_locations).sum()\n",
    "    \n",
    "    if invalid_count > 0:\n",
    "        print(f\"⚠️  发现 {invalid_count} 条locations格式异常的记录,将删除:\")\n",
    "        invalid_samples = df_cleaned[~valid_locations].head(5)\n",
    "        for idx, row in invalid_samples.iterrows():\n",
    "            loc_preview = str(row['locations'])[:100] + '...' if len(str(row['locations'])) > 100 else str(row['locations'])\n",
    "            print(f\"  行 {idx+2}: {loc_preview}\")\n",
    "        if invalid_count > 5:\n",
    "            print(f\"  ... 还有 {invalid_count-5} 条\")\n",
    "        df_cleaned = df_cleaned[valid_locations].copy()\n",
    "    else:\n",
    "        print(\"✓ 所有locations格式正确\")\n",
    "    \n",
    "    print(f\"处理后数据行数: {len(df_cleaned)}\")\n",
    "    \n",
    "    # 4. 处理times字段\n",
    "    print(\"\\n=== 处理times字段 ===\")\n",
    "    \n",
    "    def extract_date_from_date_col(date_str):\n",
    "        \"\"\"从date列提取时间,转换为yyyy-mm格式\"\"\"\n",
    "        if pd.isna(date_str) or date_str == '':\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            date_str = str(date_str).strip()\n",
    "            \n",
    "            # 尝试解析日期\n",
    "            import re\n",
    "            # 匹配yyyy-mm-dd或yyyy/mm/dd或yyyy.mm.dd格式\n",
    "            pattern = r'(\\d{4})[-/.](\\d{1,2})'\n",
    "            match = re.search(pattern, date_str)\n",
    "            \n",
    "            if match:\n",
    "                year, month = int(match.group(1)), int(match.group(2))\n",
    "                if 1900 <= year <= 2100 and 1 <= month <= 12:\n",
    "                    return f\"{year:04d}-{month:02d}\"\n",
    "            \n",
    "            return None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def process_time(time_str, date_str):\n",
    "        \"\"\"\n",
    "        处理时间字段,返回格式为yyyy-mm\n",
    "        优先使用times,异常时从date列提取\n",
    "        \"\"\"\n",
    "        # 尝试处理times\n",
    "        if pd.notna(time_str) and time_str != '':\n",
    "            try:\n",
    "                time_str = str(time_str).strip()\n",
    "                \n",
    "                # 如果是列表,只保留第一个\n",
    "                if time_str.startswith('['):\n",
    "                    time_list = ast.literal_eval(time_str)\n",
    "                    if isinstance(time_list, list) and len(time_list) > 0:\n",
    "                        time_str = str(time_list[0])\n",
    "                    else:\n",
    "                        # 列表为空,尝试从date提取\n",
    "                        extracted = extract_date_from_date_col(date_str)\n",
    "                        return extracted if extracted else None, 'from_date'\n",
    "                \n",
    "                # 验证格式 yyyy-mm\n",
    "                if len(time_str) >= 7:\n",
    "                    parts = time_str[:7].split('-')\n",
    "                    if len(parts) == 2:\n",
    "                        year, month = int(parts[0]), int(parts[1])\n",
    "                        if 1900 <= year <= 2100 and 1 <= month <= 12:\n",
    "                            return f\"{year:04d}-{month:02d}\", 'ok'\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # times为空或异常,尝试从date提取\n",
    "        extracted = extract_date_from_date_col(date_str)\n",
    "        if extracted:\n",
    "            return extracted, 'from_date'\n",
    "        else:\n",
    "            return None, 'failed'\n",
    "    \n",
    "    # 处理times\n",
    "    results = df_cleaned.apply(lambda row: process_time(row['times'], row['date']), axis=1)\n",
    "    df_cleaned['times_clean'] = results.apply(lambda x: x[0])\n",
    "    df_cleaned['time_source'] = results.apply(lambda x: x[1])\n",
    "    \n",
    "    # 统计状态\n",
    "    status_counts = df_cleaned['time_source'].value_counts()\n",
    "    print(\"\\n时间处理状态统计:\")\n",
    "    for status, count in status_counts.items():\n",
    "        status_desc = {\n",
    "            'ok': '✓ 原始times格式正确',\n",
    "            'from_date': '⚠️  从date列提取',\n",
    "            'failed': '✗ times和date都无法提取'\n",
    "        }\n",
    "        print(f\"  {status_desc.get(status, status)}: {count} 条\")\n",
    "    \n",
    "    # 删除无法提取时间的记录\n",
    "    failed_mask = df_cleaned['time_source'] == 'failed'\n",
    "    failed_count = failed_mask.sum()\n",
    "    \n",
    "    if failed_count > 0:\n",
    "        print(f\"\\n删除 {failed_count} 条无法提取时间的记录:\")\n",
    "        failed_samples = df_cleaned[failed_mask].head(5)\n",
    "        for idx, row in failed_samples.iterrows():\n",
    "            print(f\"  行 {idx+2}: times={row['times']}, date={row['date']}\")\n",
    "        if failed_count > 5:\n",
    "            print(f\"  ... 还有 {failed_count-5} 条\")\n",
    "        df_cleaned = df_cleaned[~failed_mask].copy()\n",
    "    \n",
    "    print(f\"\\n最终数据行数: {len(df_cleaned)}\")\n",
    "    \n",
    "    # 5. 保存清洗后的数据\n",
    "    if output_file is None:\n",
    "        input_path = Path(input_file)\n",
    "        output_file = input_path.parent / f\"{input_path.stem}_cleaned.xlsx\"\n",
    "    \n",
    "    print(f\"\\n保存清洗后的数据到: {output_file}\")\n",
    "    df_cleaned.to_excel(output_file, index=False)\n",
    "    \n",
    "    # 生成清洗报告\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"数据清洗摘要\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"原始记录数: {len(df)}\")\n",
    "    print(f\"清洗后记录数: {len(df_cleaned)}\")\n",
    "    print(f\"删除记录数: {len(df) - len(df_cleaned)}\")\n",
    "    print(f\"  - species为空: {empty_count}\")\n",
    "    print(f\"  - locations异常: {invalid_count}\")\n",
    "    print(f\"  - times和date都无法提取: {failed_count}\")\n",
    "    print(f\"\\n新增列(保留所有原始列):\")\n",
    "    print(f\"  - struc_addr: 从locations提取的结构化地址\")\n",
    "    print(f\"  - times_clean: 清洗后的时间(yyyy-mm格式)\")\n",
    "    print(f\"  - time_source: 时间来源标记 (ok/from_date)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "df_cleaned = clean_data('uie结果完整版.xlsx', 'output/uie清洗待整理&人工审核.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10616eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取文件: output/uie清洗待整理&人工审核.xlsx\n",
      "\n",
      "原始数据行数: 5151\n",
      "\n",
      "=== 过滤无效地址 ===\n",
      "删除包含无效关键词的地址: 396 条\n",
      "  行 31: 全国范围\n",
      "  行 57: 中国-南方地区-未指定\n",
      "  行 148: 中国\n",
      "  行 155: 中国-全国-全国\n",
      "  行 157: 浙江省-未知市-未知区, 江西省-未知市-未知区\n",
      "  行 158: ['黑龙江省-未知-未知', '吉林省-未知-未知', '辽宁省-未知-未知', '湖北省-未知-未知', '江苏省-未知-未知', '山东省-未知-未知', '浙江省-未知-未知', '江西省-未知-未知', '安徽省-未知-未知']\n",
      "  行 165: 吉林省-长春市-未明确区\n",
      "  行 176: 中国\n",
      "  行 180: 辽宁省-沈阳市-未知区\n",
      "  行 187: 中国\n",
      "  ... 还有 386 条\n",
      "过滤后数据行数: 4755\n",
      "\n",
      "=== 过滤国外地址 ===\n",
      "删除国外地址: 113 条\n",
      "  行 25: 华南地区\n",
      "  行 47: 西南地区\n",
      "  行 67: 墨西哥-东南部\n",
      "  行 126: 东北地区\n",
      "  行 184: 俄罗斯-莫斯科-莫斯科\n",
      "  行 248: 沿海各省-沿海各市-沿海各区\n",
      "  行 254: 沿海各省-沿海各市-沿海各区\n",
      "  行 268: 美国-大西洋沿岸\n",
      "  行 291: 环渤海地区\n",
      "  行 414: 沿海各省市-沿海各市-沿海各区\n",
      "  ... 还有 103 条\n",
      "过滤后数据行数: 4642\n",
      "\n",
      "=== 处理struc_addr字段 ===\n",
      "\n",
      "地址处理统计:\n",
      "状态                   数量         说明\n",
      "------------------------------------------------------------\n",
      "✓ 标准格式(省-市-区)        4282       (92.2%)\n",
      "⚠️  不完整但已保留(省或省-市)   358        (7.7%)\n",
      "⚠️  多地址截取前3段         2          (0.0%)\n",
      "\n",
      "处理的地址样例(前10条):\n",
      "\n",
      "  行 61:\n",
      "    原地址: 海南省-儋州市\n",
      "    处理后: 海南省-儋州市\n",
      "    说明: 省-市(已保留)\n",
      "\n",
      "  行 70:\n",
      "    原地址: 广西壮族自治区-百色市\n",
      "    处理后: 广西壮族自治区-百色市\n",
      "    说明: 省-市(已保留)\n",
      "\n",
      "  行 73:\n",
      "    原地址: 四川省-凉山彝族自治州\n",
      "    处理后: 四川省-凉山彝族自治州\n",
      "    说明: 省-市(已保留)\n",
      "\n",
      "  行 83:\n",
      "    原地址: 辽宁省\n",
      "    处理后: 辽宁省\n",
      "    说明: 仅有省级(已保留)\n",
      "\n",
      "  行 88:\n",
      "    原地址: 新疆维吾尔自治区-伊犁哈萨克自治州\n",
      "    处理后: 新疆维吾尔自治区-伊犁哈萨克自治州\n",
      "    说明: 省-市(已保留)\n",
      "\n",
      "  行 93:\n",
      "    原地址: 四川省-自贡市\n",
      "    处理后: 四川省-自贡市\n",
      "    说明: 省-市(已保留)\n",
      "\n",
      "  行 115:\n",
      "    原地址: 四川省\n",
      "    处理后: 四川省\n",
      "    说明: 仅有省级(已保留)\n",
      "\n",
      "  行 131:\n",
      "    原地址: 新疆维吾尔自治区-伊犁哈萨克自治州\n",
      "    处理后: 新疆维吾尔自治区-伊犁哈萨克自治州\n",
      "    说明: 省-市(已保留)\n",
      "\n",
      "  行 147:\n",
      "    原地址: 辽宁省-沈阳市\n",
      "    处理后: 辽宁省-沈阳市\n",
      "    说明: 省-市(已保留)\n",
      "\n",
      "  行 196:\n",
      "    原地址: 四川省\n",
      "    处理后: 四川省\n",
      "    说明: 仅有省级(已保留)\n",
      "\n",
      "  ... 还有 350 条处理的地址\n",
      "\n",
      "最终数据行数: 4642\n",
      "\n",
      "\n",
      "保存验证后的数据到: output\\uie清洗待整理&人工审核_validated2.xlsx\n",
      "\n",
      "============================================================\n",
      "地址处理摘要\n",
      "============================================================\n",
      "原始记录数: 5151\n",
      "处理后记录数: 4642\n",
      "删除记录数: 509\n",
      "  - 包含无效关键词: 396 条\n",
      "  - 国外地址: 113 条\n",
      "  - 地址为空: 0 条\n",
      "\n",
      "地址格式统计:\n",
      "  标准格式(省-市-区): 4282 条\n",
      "  不完整但保留(省/省-市): 358 条\n",
      "  多地址截取: 2 条\n",
      "\n",
      "新增列:\n",
      "  - struc_addr_clean: 处理后的标准地址\n",
      "  - addr_status: 地址状态 (valid/incomplete_kept/trimmed)\n",
      "  - addr_note: 处理说明\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "地址处理和验证脚本 - Jupyter Notebook版本\n",
    "功能:\n",
    "1. 过滤无效地址(包含\"中国\"、\"全国\"、\"未指定\"等关键词)\n",
    "2. 过滤国外地址\n",
    "3. 处理多地址(取第一个)\n",
    "4. 标准化地址格式(提取前3段为省-市-区)\n",
    "5. 保留不完整地址(省或省-市)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def validate_struc_addr(input_file, output_file=None):\n",
    "    \"\"\"\n",
    "    验证struc_addr字段的有效性\n",
    "    \n",
    "    Args:\n",
    "        input_file: 输入xlsx文件路径\n",
    "        output_file: 输出xlsx文件路径(可选,默认为input_validated.xlsx)\n",
    "    \n",
    "    Returns:\n",
    "        df_validated: 验证后的DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # 读取Excel文件\n",
    "    print(f\"读取文件: {input_file}\")\n",
    "    df = pd.read_excel(input_file)\n",
    "    \n",
    "    # 验证列名\n",
    "    if 'struc_addr' not in df.columns:\n",
    "        raise ValueError(\"文件中缺少 'struc_addr' 列\")\n",
    "    \n",
    "    print(f\"\\n原始数据行数: {len(df)}\")\n",
    "    \n",
    "    df_validated = df.copy()\n",
    "    \n",
    "    # 1. 过滤无效地址关键词\n",
    "    print(\"\\n=== 过滤无效地址 ===\")\n",
    "    \n",
    "    # 定义需要过滤的关键词\n",
    "    invalid_keywords = [\n",
    "        '中国', '全国范围', '全国', '未指定', '未提及', '未明确提及', \n",
    "        '未明确', '未知', '不详', '待定', 'unknown', 'Unknown'\n",
    "    ]\n",
    "    \n",
    "    def contains_invalid_keyword(addr_str):\n",
    "        \"\"\"检查地址是否包含无效关键词\"\"\"\n",
    "        if pd.isna(addr_str) or addr_str == '':\n",
    "            return False\n",
    "        \n",
    "        addr_str = str(addr_str)\n",
    "        for keyword in invalid_keywords:\n",
    "            if keyword in addr_str:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    invalid_keyword_mask = df_validated['struc_addr'].apply(contains_invalid_keyword)\n",
    "    invalid_keyword_count = invalid_keyword_mask.sum()\n",
    "    \n",
    "    if invalid_keyword_count > 0:\n",
    "        print(f\"删除包含无效关键词的地址: {invalid_keyword_count} 条\")\n",
    "        invalid_samples = df_validated[invalid_keyword_mask].head(10)\n",
    "        for idx, row in invalid_samples.iterrows():\n",
    "            print(f\"  行 {idx+2}: {row['struc_addr']}\")\n",
    "        if invalid_keyword_count > 10:\n",
    "            print(f\"  ... 还有 {invalid_keyword_count-10} 条\")\n",
    "        df_validated = df_validated[~invalid_keyword_mask].copy()\n",
    "    else:\n",
    "        print(\"✓ 没有包含无效关键词的地址\")\n",
    "    \n",
    "    print(f\"过滤后数据行数: {len(df_validated)}\")\n",
    "    \n",
    "    # 2. 过滤国外地址\n",
    "    print(\"\\n=== 过滤国外地址 ===\")\n",
    "    \n",
    "    # 中国省级行政区列表(包括省、自治区、直辖市、特别行政区)\n",
    "    china_provinces = [\n",
    "        '北京市', '天津市', '上海市', '重庆市',  # 直辖市\n",
    "        '河北省', '山西省', '辽宁省', '吉林省', '黑龙江省',  # 省\n",
    "        '江苏省', '浙江省', '安徽省', '福建省', '江西省', '山东省',\n",
    "        '河南省', '湖北省', '湖南省', '广东省', '海南省',\n",
    "        '四川省', '贵州省', '云南省', '陕西省', '甘肃省', '青海省', '台湾省',\n",
    "        '内蒙古自治区', '广西壮族自治区', '西藏自治区', '宁夏回族自治区', '新疆维吾尔自治区',  # 自治区\n",
    "        '香港特别行政区', '澳门特别行政区',  # 特别行政区\n",
    "        # 简称形式\n",
    "        '北京', '天津', '上海', '重庆',\n",
    "        '河北', '山西', '辽宁', '吉林', '黑龙江',\n",
    "        '江苏', '浙江', '安徽', '福建', '江西', '山东',\n",
    "        '河南', '湖北', '湖南', '广东', '海南',\n",
    "        '四川', '贵州', '云南', '陕西', '甘肃', '青海', '台湾',\n",
    "        '内蒙古', '广西', '西藏', '宁夏', '新疆',\n",
    "        '香港', '澳门'\n",
    "    ]\n",
    "    \n",
    "    def is_china_address(addr_str):\n",
    "        \"\"\"检查是否为中国地址\"\"\"\n",
    "        if pd.isna(addr_str) or addr_str == '':\n",
    "            return False\n",
    "        \n",
    "        addr_str = str(addr_str)\n",
    "        \n",
    "        # 检查第一段是否为中国省份\n",
    "        first_part = addr_str.split('-')[0] if '-' in addr_str else addr_str\n",
    "        \n",
    "        return first_part in china_provinces\n",
    "    \n",
    "    china_mask = df_validated['struc_addr'].apply(is_china_address)\n",
    "    foreign_count = (~china_mask).sum()\n",
    "    \n",
    "    if foreign_count > 0:\n",
    "        print(f\"删除国外地址: {foreign_count} 条\")\n",
    "        foreign_samples = df_validated[~china_mask].head(10)\n",
    "        for idx, row in foreign_samples.iterrows():\n",
    "            print(f\"  行 {idx+2}: {row['struc_addr']}\")\n",
    "        if foreign_count > 10:\n",
    "            print(f\"  ... 还有 {foreign_count-10} 条\")\n",
    "        df_validated = df_validated[china_mask].copy()\n",
    "    else:\n",
    "        print(\"✓ 没有国外地址\")\n",
    "    \n",
    "    print(f\"过滤后数据行数: {len(df_validated)}\")\n",
    "    \n",
    "    # 3. 处理和验证地址有效性\n",
    "    print(\"\\n=== 处理struc_addr字段 ===\")\n",
    "    \n",
    "    def process_address(addr_str):\n",
    "        \"\"\"\n",
    "        处理地址:\n",
    "        1. 如果包含多个地址(逗号或分号分隔),取第一个\n",
    "        2. 提取标准的xx-xx-xx格式(前3段)\n",
    "        返回: (processed_addr, status, note)\n",
    "        \"\"\"\n",
    "        if pd.isna(addr_str) or addr_str == '':\n",
    "            return None, 'empty', '地址为空'\n",
    "        \n",
    "        addr_str = str(addr_str).strip()\n",
    "        \n",
    "        # 处理多地址情况(用逗号、分号、顿号分隔)\n",
    "        if ',' in addr_str or '，' in addr_str or ';' in addr_str or '；' in addr_str or '、' in addr_str:\n",
    "            # 分割并取第一个\n",
    "            for sep in [',', '，', ';', '；', '、']:\n",
    "                if sep in addr_str:\n",
    "                    parts = addr_str.split(sep)\n",
    "                    addr_str = parts[0].strip()\n",
    "                    break\n",
    "        \n",
    "        # 检查是否以-开头或结尾\n",
    "        if addr_str.startswith('-') or addr_str.endswith('-'):\n",
    "            addr_str = addr_str.strip('-')\n",
    "        \n",
    "        # 分割地址\n",
    "        parts = addr_str.split('-')\n",
    "        \n",
    "        # 移除空段\n",
    "        parts = [part.strip() for part in parts if part.strip()]\n",
    "        \n",
    "        if len(parts) == 0:\n",
    "            return None, 'empty', '地址为空'\n",
    "        elif len(parts) == 1:\n",
    "            # 只有一段,保留原样(可能是省级)\n",
    "            return parts[0], 'incomplete_kept', '仅有省级(已保留)'\n",
    "        elif len(parts) == 2:\n",
    "            # 两段,保留原样(省-市)\n",
    "            return '-'.join(parts), 'incomplete_kept', '省-市(已保留)'\n",
    "        elif len(parts) == 3:\n",
    "            # 标准格式\n",
    "            return '-'.join(parts), 'valid', '标准格式'\n",
    "        else:\n",
    "            # 超过3段,只取前3段\n",
    "            return '-'.join(parts[:3]), 'trimmed', f'多段地址(从{len(parts)}段截取前3段)'\n",
    "    \n",
    "    # 处理地址\n",
    "    process_results = df_validated['struc_addr'].apply(process_address)\n",
    "    df_validated['struc_addr_clean'] = process_results.apply(lambda x: x[0])\n",
    "    df_validated['addr_status'] = process_results.apply(lambda x: x[1])\n",
    "    df_validated['addr_note'] = process_results.apply(lambda x: x[2])\n",
    "    \n",
    "    # 删除处理后仍为空的地址\n",
    "    empty_mask = df_validated['struc_addr_clean'].isna()\n",
    "    empty_count = empty_mask.sum()\n",
    "    \n",
    "    if empty_count > 0:\n",
    "        print(f\"删除处理后仍为空的地址: {empty_count} 条\")\n",
    "        df_validated = df_validated[~empty_mask].copy()\n",
    "    \n",
    "    # 统计结果\n",
    "    status_counts = df_validated['addr_status'].value_counts()\n",
    "    \n",
    "    print(\"\\n地址处理统计:\")\n",
    "    print(f\"{'状态':<20} {'数量':<10} {'说明'}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    status_desc = {\n",
    "        'valid': '✓ 标准格式(省-市-区)',\n",
    "        'incomplete_kept': '⚠️  不完整但已保留(省或省-市)',\n",
    "        'trimmed': '⚠️  多地址截取前3段',\n",
    "        'empty': '✗ 地址为空'\n",
    "    }\n",
    "    \n",
    "    for status in ['valid', 'incomplete_kept', 'trimmed', 'empty']:\n",
    "        if status in status_counts:\n",
    "            count = status_counts[status]\n",
    "            desc = status_desc.get(status, status)\n",
    "            percentage = count / len(df_validated) * 100\n",
    "            print(f\"{desc:<20} {count:<10} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # 显示处理样例\n",
    "    processed_mask = df_validated['addr_status'].isin(['incomplete_kept', 'trimmed'])\n",
    "    processed_count = processed_mask.sum()\n",
    "    \n",
    "    if processed_count > 0:\n",
    "        print(f\"\\n处理的地址样例(前10条):\")\n",
    "        processed_samples = df_validated[processed_mask].head(10)\n",
    "        \n",
    "        for idx, row in processed_samples.iterrows():\n",
    "            print(f\"\\n  行 {idx+2}:\")\n",
    "            print(f\"    原地址: {row['struc_addr']}\")\n",
    "            print(f\"    处理后: {row['struc_addr_clean']}\")\n",
    "            print(f\"    说明: {row['addr_note']}\")\n",
    "        \n",
    "        if processed_count > 10:\n",
    "            print(f\"\\n  ... 还有 {processed_count-10} 条处理的地址\")\n",
    "    \n",
    "    print(f\"\\n最终数据行数: {len(df_validated)}\")\n",
    "    \n",
    "    # 保存验证后的数据\n",
    "    if output_file is None:\n",
    "        input_path = Path(input_file)\n",
    "        output_file = input_path.parent / f\"{input_path.stem}_validated.xlsx\"\n",
    "    \n",
    "    print(f\"\\n\\n保存验证后的数据到: {output_file}\")\n",
    "    df_validated.to_excel(output_file, index=False)\n",
    "    \n",
    "    # 生成验证报告\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"地址处理摘要\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"原始记录数: {len(df)}\")\n",
    "    print(f\"处理后记录数: {len(df_validated)}\")\n",
    "    print(f\"删除记录数: {len(df) - len(df_validated)}\")\n",
    "    print(f\"  - 包含无效关键词: {invalid_keyword_count} 条\")\n",
    "    print(f\"  - 国外地址: {foreign_count} 条\")\n",
    "    print(f\"  - 地址为空: {empty_count} 条\")\n",
    "    print(f\"\\n地址格式统计:\")\n",
    "    print(f\"  标准格式(省-市-区): {status_counts.get('valid', 0)} 条\")\n",
    "    print(f\"  不完整但保留(省/省-市): {status_counts.get('incomplete_kept', 0)} 条\")\n",
    "    print(f\"  多地址截取: {status_counts.get('trimmed', 0)} 条\")\n",
    "    print(f\"\\n新增列:\")\n",
    "    print(f\"  - struc_addr_clean: 处理后的标准地址\")\n",
    "    print(f\"  - addr_status: 地址状态 (valid/incomplete_kept/trimmed)\")\n",
    "    print(f\"  - addr_note: 处理说明\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return df_validated\n",
    "\n",
    "df_validated = validate_struc_addr('output/uie清洗待整理&人工审核.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5819860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取文件: output/uie清洗待整理&人工审核_validated.xlsx\n",
      "\n",
      "原始数据行数: 4642\n",
      "\n",
      "=== 处理species字段并识别入侵物种 ===\n",
      "\n",
      "入侵物种识别统计:\n",
      "类别                        数量         比例\n",
      "------------------------------------------------------------\n",
      "包含入侵物种                    4334       (93.4%)\n",
      "不包含入侵物种                   308        (6.6%)\n",
      "总计                        4642       (100.0%)\n",
      "\n",
      "原始缺失率: 6.64%\n",
      "\n",
      "删除不包含入侵物种的记录: 308 条\n",
      "  行 24: 小花宽叶十万错\n",
      "  行 25: 扶桑绵粉蚧\n",
      "  行 29: 三裂叶蟛蜞菊\n",
      "  行 31: 三裂叶蟛蜞菊\n",
      "  行 57: 夏孢锈菌, 粉孢属, 泽兰尾孢, 刺盘孢, 交链孢, 长蠕孢霉, 弯孢霉, 夏孢锈菌, 黑孢霉\n",
      "  行 59: Ramularia caricis, Ramularia concomitans\n",
      "  行 106: 三裂叶豚草锈菌\n",
      "  行 166: 夜蛾, 象甲, 叶甲, 瘿蝇, 长角象甲, 豚草条纹叶甲, 白锈菌, 万寿菊叶斑病菌, 豚草木质部抑制细菌, 三裂叶豚草锈菌, 多年生禾本科牧草, 紫穗槐\n",
      "  行 173: 苍耳属非中国种Xanthium spp.non-Chinese, 三裂叶豚草Ambrosia trifi da Linn., 墨天牛非中国种Monochamus spp.non-Chinese\n",
      "  行 196: 豚草属Ambrosia\n",
      "  ... 还有 298 条\n",
      "\n",
      "删除后数据行数: 4334\n",
      "\n",
      "识别到入侵物种的样例(前10条):\n",
      "\n",
      "  行 2:\n",
      "    原始species: 三叶鬼针草, 马唐\n",
      "    入侵物种: 三叶鬼针草\n",
      "\n",
      "  行 3:\n",
      "    原始species: 三叶鬼针草\n",
      "    入侵物种: 三叶鬼针草\n",
      "\n",
      "  行 4:\n",
      "    原始species: 三叶鬼针草\n",
      "    入侵物种: 三叶鬼针草\n",
      "\n",
      "  行 5:\n",
      "    原始species: 三叶鬼针草\n",
      "    入侵物种: 三叶鬼针草\n",
      "\n",
      "  行 6:\n",
      "    原始species: 三叶鬼针草, 鬼针草\n",
      "    入侵物种: 三叶鬼针草, 鬼针草\n",
      "\n",
      "  行 7:\n",
      "    原始species: 三叶鬼针草\n",
      "    入侵物种: 三叶鬼针草\n",
      "\n",
      "  行 8:\n",
      "    原始species: 三叶鬼针草\n",
      "    入侵物种: 三叶鬼针草\n",
      "\n",
      "  行 9:\n",
      "    原始species: 三叶鬼针草\n",
      "    入侵物种: 三叶鬼针草\n",
      "\n",
      "  行 10:\n",
      "    原始species: 三叶鬼针草\n",
      "    入侵物种: 三叶鬼针草\n",
      "\n",
      "  行 11:\n",
      "    原始species: 香茅, 胜红蓟, 三叶鬼针草\n",
      "    入侵物种: 胜红蓟, 三叶鬼针草\n",
      "\n",
      "  ... 还有 4324 条\n",
      "\n",
      "\n",
      "入侵物种出现频次统计(Top 15):\n",
      "\n",
      "物种名称                 出现次数      \n",
      "----------------------------------------\n",
      "互花米草                 852       \n",
      "野燕麦                  765       \n",
      "紫茎泽兰                 597       \n",
      "空心莲子草                412       \n",
      "豚草                   363       \n",
      "加拿大一枝黄花              350       \n",
      "薇甘菊                  331       \n",
      "三裂叶豚草                270       \n",
      "飞机草                  247       \n",
      "假高粱                  165       \n",
      "三叶鬼针草                114       \n",
      "假臭草                  112       \n",
      "马缨丹                  90        \n",
      "五爪金龙                 84        \n",
      "黄顶菊                  76        \n",
      "\n",
      "\n",
      "保存处理后的数据到: output\\uie清洗待整理&人工审核_validated_species_processed.xlsx\n",
      "\n",
      "============================================================\n",
      "物种处理摘要\n",
      "============================================================\n",
      "原始记录数: 4642\n",
      "处理后记录数: 4334\n",
      "删除记录数: 308\n",
      "  - 不包含入侵物种: 308 条\n",
      "\n",
      "原始缺失率: 6.64%\n",
      "最终保留: 100% 包含入侵物种\n",
      "\n",
      "新增列:\n",
      "  - alien_species: 识别到的入侵物种(逗号分隔)\n",
      "  - all_species_parsed: 解析后的所有物种列表\n",
      "\n",
      "入侵物种名录共包含: 39 个物种\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "物种处理和入侵物种识别脚本 - Jupyter Notebook版本\n",
    "功能:\n",
    "1. 解析species字段中的多物种(用逗号分隔)\n",
    "2. 根据入侵物种名录匹配入侵物种\n",
    "3. 删除不包含入侵物种的记录\n",
    "4. 统计原始缺失率和入侵物种频次\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def process_species(input_file, output_file=None):\n",
    "    \"\"\"\n",
    "    处理species字段并识别入侵物种\n",
    "    \n",
    "    Args:\n",
    "        input_file: 输入xlsx文件路径\n",
    "        output_file: 输出xlsx文件路径(可选,默认为input_species_processed.xlsx)\n",
    "    \n",
    "    Returns:\n",
    "        df_processed: 处理后的DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # 入侵物种名录(根据附表1)\n",
    "    alien_species_dict = {\n",
    "        '紫茎泽兰': 'Ageratina adenophora',\n",
    "        '藿香蓟': 'Ageratum conyzoides',\n",
    "        '胜红蓟': 'Ageratum conyzoides',  # 藿香蓟的别名\n",
    "        '空心莲子草': 'Alternanthera philoxeroides',\n",
    "        '长芒苋': 'Amaranthus palmeri',\n",
    "        '刺苋': 'Amaranthus spinosus',\n",
    "        '豚草': 'Ambrosia artemisiifolia',\n",
    "        '三裂叶豚草': 'Ambrosia trifida',\n",
    "        '落葵薯': 'Anredera cordifolia',\n",
    "        '野燕麦': 'Avena fatua',\n",
    "        '三叶鬼针草': 'Bidens pilosa',\n",
    "        '鬼针草': 'Bidens pilosa',  # 三叶鬼针草的简称\n",
    "        '水盾草': 'Cabomba caroliniana',\n",
    "        '长刺蒺藜草': 'Cenchrus longispinus',\n",
    "        '飞机草': 'Chromolaena odorata',\n",
    "        '凤眼蓝': 'Eichhornia crassipes',\n",
    "        '水葫芦': 'Eichhornia crassipes',  # 凤眼蓝的别名\n",
    "        '小蓬草': 'Erigeron canadensis',\n",
    "        '苏门白酒草': 'Erigeron sumatrensis',\n",
    "        '黄顶菊': 'Flaveria bidentis',\n",
    "        '五爪金龙': 'Ipomoea cairica',\n",
    "        '假苍耳': 'Cyclachaena xanthiifolia',\n",
    "        '马缨丹': 'Lantana camara',\n",
    "        '毒莴苣': 'Lactuca serriola',\n",
    "        '薇甘菊': 'Mikania micrantha',\n",
    "        '小花蔓泽兰': 'Mikania micrantha',  # 薇甘菊的别名\n",
    "        '光荚含羞草': 'Mimosa bimucronata',\n",
    "        '银胶菊': 'Parthenium hysterophorus',\n",
    "        '垂序商陆': 'Phytolacca americana',\n",
    "        '美洲商陆': 'Phytolacca americana',  # 垂序商陆的别名\n",
    "        '大薸': 'Pistia stratiotes',\n",
    "        '假臭草': 'Praxelis clematidea',\n",
    "        '刺果瓜': 'Sicyos angulatus',\n",
    "        '黄花刺茄': 'Solanum rostratum',\n",
    "        '加拿大一枝黄花': 'Solidago canadensis',\n",
    "        '一枝黄花': 'Solidago canadensis',  # 加拿大一枝黄花的简称\n",
    "        '假高粱': 'Sorghum halepense',\n",
    "        '互花米草': 'Spartina alterniflora',\n",
    "        '刺苍耳': 'Xanthium spinosum',\n",
    "    }\n",
    "    \n",
    "    # 读取Excel文件\n",
    "    print(f\"读取文件: {input_file}\")\n",
    "    df = pd.read_excel(input_file)\n",
    "    \n",
    "    # 验证列名\n",
    "    if 'species' not in df.columns:\n",
    "        raise ValueError(\"文件中缺少 'species' 列\")\n",
    "    \n",
    "    print(f\"\\n原始数据行数: {len(df)}\")\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # 处理species字段\n",
    "    print(\"\\n=== 处理species字段并识别入侵物种 ===\")\n",
    "    \n",
    "    def extract_alien_species(species_str):\n",
    "        \"\"\"\n",
    "        从species字符串中提取入侵物种\n",
    "        返回: (alien_species_list, all_species_list)\n",
    "        \"\"\"\n",
    "        if pd.isna(species_str) or species_str == '':\n",
    "            return [], []\n",
    "        \n",
    "        species_str = str(species_str).strip()\n",
    "        \n",
    "        # 分割物种(用逗号、顿号、分号等分隔)\n",
    "        separators = [',', '，', '、', ';', '；']\n",
    "        for sep in separators:\n",
    "            if sep in species_str:\n",
    "                species_list = [s.strip() for s in species_str.split(sep) if s.strip()]\n",
    "                break\n",
    "        else:\n",
    "            # 没有分隔符,单个物种\n",
    "            species_list = [species_str]\n",
    "        \n",
    "        # 匹配入侵物种\n",
    "        alien_species = []\n",
    "        for species in species_list:\n",
    "            if species in alien_species_dict:\n",
    "                if alien_species_dict[species] is not None:\n",
    "                    alien_species.append(species)\n",
    "        \n",
    "        return alien_species, species_list\n",
    "    \n",
    "    # 提取入侵物种\n",
    "    extract_results = df_processed['species'].apply(extract_alien_species)\n",
    "    df_processed['alien_species'] = extract_results.apply(\n",
    "        lambda x: ', '.join(x[0]) if x[0] else None\n",
    "    )\n",
    "    df_processed['all_species_parsed'] = extract_results.apply(\n",
    "        lambda x: ', '.join(x[1]) if x[1] else None\n",
    "    )\n",
    "    \n",
    "    # 统计结果\n",
    "    has_alien = df_processed['alien_species'].notna()\n",
    "    alien_count = has_alien.sum()\n",
    "    no_alien_count = (~has_alien).sum()\n",
    "    \n",
    "    total_count = len(df_processed)\n",
    "    missing_rate = (no_alien_count / total_count * 100) if total_count > 0 else 0\n",
    "    \n",
    "    print(f\"\\n入侵物种识别统计:\")\n",
    "    print(f\"{'类别':<25} {'数量':<10} {'比例'}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'包含入侵物种':<25} {alien_count:<10} ({alien_count/total_count*100:.1f}%)\")\n",
    "    print(f\"{'不包含入侵物种':<25} {no_alien_count:<10} ({missing_rate:.1f}%)\")\n",
    "    print(f\"{'总计':<25} {total_count:<10} (100.0%)\")\n",
    "    \n",
    "    print(f\"\\n原始缺失率: {missing_rate:.2f}%\")\n",
    "    \n",
    "    # 删除不包含入侵物种的记录\n",
    "    if no_alien_count > 0:\n",
    "        print(f\"\\n删除不包含入侵物种的记录: {no_alien_count} 条\")\n",
    "        no_alien_samples = df_processed[~has_alien].head(10)\n",
    "        for idx, row in no_alien_samples.iterrows():\n",
    "            print(f\"  行 {idx+2}: {row['species']}\")\n",
    "        if no_alien_count > 10:\n",
    "            print(f\"  ... 还有 {no_alien_count-10} 条\")\n",
    "        \n",
    "        df_processed = df_processed[has_alien].copy()\n",
    "        print(f\"\\n删除后数据行数: {len(df_processed)}\")\n",
    "    else:\n",
    "        print(\"\\n✓ 所有记录都包含入侵物种\")\n",
    "    \n",
    "    # 显示识别到的入侵物种样例\n",
    "    if len(df_processed) > 0:\n",
    "        print(f\"\\n识别到入侵物种的样例(前10条):\")\n",
    "        alien_samples = df_processed.head(10)\n",
    "        \n",
    "        for idx, row in alien_samples.iterrows():\n",
    "            print(f\"\\n  行 {idx+2}:\")\n",
    "            print(f\"    原始species: {row['species']}\")\n",
    "            print(f\"    入侵物种: {row['alien_species']}\")\n",
    "        \n",
    "        if len(df_processed) > 10:\n",
    "            print(f\"\\n  ... 还有 {len(df_processed)-10} 条\")\n",
    "    \n",
    "    # 统计各入侵物种出现次数\n",
    "    if len(df_processed) > 0:\n",
    "        print(\"\\n\\n入侵物种出现频次统计(Top 15):\")\n",
    "        all_aliens = []\n",
    "        for aliens_str in df_processed['alien_species'].dropna():\n",
    "            all_aliens.extend([s.strip() for s in aliens_str.split(',')])\n",
    "        \n",
    "        alien_freq = pd.Series(all_aliens).value_counts()\n",
    "        print(f\"\\n{'物种名称':<20} {'出现次数':<10}\")\n",
    "        print(\"-\" * 40)\n",
    "        for species, count in alien_freq.head(15).items():\n",
    "            print(f\"{species:<20} {count:<10}\")\n",
    "    \n",
    "    # 保存处理后的数据\n",
    "    if output_file is None:\n",
    "        input_path = Path(input_file)\n",
    "        output_file = input_path.parent / f\"{input_path.stem}_species_processed.xlsx\"\n",
    "    \n",
    "    print(f\"\\n\\n保存处理后的数据到: {output_file}\")\n",
    "    df_processed.to_excel(output_file, index=False)\n",
    "    \n",
    "    # 生成处理报告\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"物种处理摘要\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"原始记录数: {total_count}\")\n",
    "    print(f\"处理后记录数: {len(df_processed)}\")\n",
    "    print(f\"删除记录数: {no_alien_count}\")\n",
    "    print(f\"  - 不包含入侵物种: {no_alien_count} 条\")\n",
    "    print(f\"\\n原始缺失率: {missing_rate:.2f}%\")\n",
    "    print(f\"最终保留: 100% 包含入侵物种\")\n",
    "    print(f\"\\n新增列:\")\n",
    "    print(f\"  - alien_species: 识别到的入侵物种(逗号分隔)\")\n",
    "    print(f\"  - all_species_parsed: 解析后的所有物种列表\")\n",
    "    print(f\"\\n入侵物种名录共包含: {len([k for k, v in alien_species_dict.items() if v is not None])} 个物种\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "df_processed = process_species('output/uie清洗待整理&人工审核_validated.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "056783fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取文件: output/backups/backup_final_04334_retry_final.xlsx\n",
      "\n",
      "数据行数: 4334\n",
      "有效坐标数: 4334\n",
      "\n",
      "物种数量: 38\n",
      "前10种物种:\n",
      "  互花米草: 852\n",
      "  野燕麦: 765\n",
      "  紫茎泽兰: 597\n",
      "  空心莲子草: 412\n",
      "  豚草: 363\n",
      "  加拿大一枝黄花: 350\n",
      "  薇甘菊: 331\n",
      "  三裂叶豚草: 270\n",
      "  飞机草: 247\n",
      "  假高粱: 165\n",
      "\n",
      "图片将保存到: output\\backups\\visualizations\n",
      "正在加载中国地图底图...\n",
      "✓ 地图加载成功\n",
      "\n",
      "生成总体分布图...\n",
      "✓ 已保存: overall_distribution.png\n",
      "\n",
      "生成时间序列累积图...\n",
      "✓ 已保存: cumulative_trend.png\n",
      "\n",
      "生成物种-年份热力图...\n",
      "✓ 已保存: species_year_heatmap.png\n",
      "\n",
      "生成Top物种地理分布对比图...\n",
      "✓ 已保存: top5_species_comparison.png\n",
      "\n",
      "生成密度分布热力图...\n",
      "✓ 已保存: density_distribution.png\n",
      "\n",
      "============================================================\n",
      "可视化完成\n",
      "============================================================\n",
      "总记录数: 4334\n",
      "时间范围: 1911 - 2023\n",
      "可视化物种数: 15\n",
      "输出目录: output\\backups\\visualizations\n",
      "\n",
      "生成的图片:\n",
      "  1. overall_distribution.png - 总体空间分布\n",
      "  2. distribution_by_year.png - 按年份分布\n",
      "  3. cumulative_trend.png - 累积趋势\n",
      "  4. species_year_heatmap.png - 时间热力图\n",
      "  5. top5_species_comparison.png - Top5物种对比\n",
      "  6. density_distribution.png - 密度分布图 (新增)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "入侵物种时空分布可视化脚本 - 改进版\n",
    "功能:\n",
    "1. 使用真实的lng, lat经纬度数据\n",
    "2. 添加中国地图作为底图\n",
    "3. 绘制不同物种的空间分布图(不同颜色)\n",
    "4. 绘制时间演变动画/多图\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.patches import Circle, Polygon\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import json\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n",
    "\n",
    "\n",
    "def load_china_map():\n",
    "    \"\"\"\n",
    "    加载中国地图底图\n",
    "    \"\"\"\n",
    "    china_url = \"https://geo.datav.aliyun.com/areas_v3/bound/100000.json\"\n",
    "    try:\n",
    "        print(\"正在加载中国地图底图...\")\n",
    "        response = requests.get(china_url, timeout=10)\n",
    "        china_geojson = response.json()\n",
    "        print(\"✓ 地图加载成功\")\n",
    "        return china_geojson\n",
    "    except Exception as e:\n",
    "        print(f\"警告：无法加载在线地图 ({e})，将仅绘制坐标轴。\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def plot_china_basemap(ax, china_geojson):\n",
    "    \"\"\"\n",
    "    在指定axes上绘制中国地图底图\n",
    "    \"\"\"\n",
    "    if china_geojson is None:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        features = china_geojson.get('features', [])\n",
    "        for feature in features:\n",
    "            geometry = feature.get('geometry', {})\n",
    "            geo_type = geometry.get('type', '')\n",
    "            coordinates = geometry.get('coordinates', [])\n",
    "            \n",
    "            if geo_type == 'Polygon':\n",
    "                for poly_coords in coordinates:\n",
    "                    poly = Polygon(poly_coords, facecolor='#ebebeb', \n",
    "                                 edgecolor='#999999', alpha=0.5, linewidth=0.5)\n",
    "                    ax.add_patch(poly)\n",
    "            elif geo_type == 'MultiPolygon':\n",
    "                for multi_poly in coordinates:\n",
    "                    for poly_coords in multi_poly:\n",
    "                        poly = Polygon(poly_coords, facecolor='#ebebeb', \n",
    "                                     edgecolor='#999999', alpha=0.5, linewidth=0.5)\n",
    "                        ax.add_patch(poly)\n",
    "    except Exception as e:\n",
    "        print(f\"绘制地图时出错: {e}\")\n",
    "\n",
    "\n",
    "def visualize_species_distribution(input_file, output_dir=None):\n",
    "    \"\"\"\n",
    "    可视化入侵物种的时空分布\n",
    "    \n",
    "    Args:\n",
    "        input_file: 输入xlsx文件路径\n",
    "        output_dir: 输出图片目录(可选)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 读取数据\n",
    "    print(f\"读取文件: {input_file}\")\n",
    "    df = pd.read_excel(input_file)\n",
    "    \n",
    "    # 验证列名\n",
    "    required_cols = ['alien_species', 'times_clean', 'lng', 'lat']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"文件中缺少必需的列: {missing_cols}\")\n",
    "    \n",
    "    print(f\"\\n数据行数: {len(df)}\")\n",
    "    \n",
    "    # 使用真实经纬度\n",
    "    df['longitude'] = pd.to_numeric(df['lng'], errors='coerce')\n",
    "    df['latitude'] = pd.to_numeric(df['lat'], errors='coerce')\n",
    "    \n",
    "    # 移除坐标缺失的记录\n",
    "    valid_coords = df['longitude'].notna() & df['latitude'].notna()\n",
    "    df_plot = df[valid_coords].copy()\n",
    "    print(f\"有效坐标数: {len(df_plot)}\")\n",
    "    \n",
    "    # 解析时间\n",
    "    df_plot['year'] = df_plot['times_clean'].apply(lambda x: str(x)[:4] if pd.notna(x) else None)\n",
    "    df_plot['year_month'] = df_plot['times_clean']\n",
    "    \n",
    "    # 统计物种\n",
    "    all_species = []\n",
    "    for species_str in df_plot['alien_species'].dropna():\n",
    "        all_species.extend([s.strip() for s in str(species_str).split(',')])\n",
    "    \n",
    "    species_counts = pd.Series(all_species).value_counts()\n",
    "    print(f\"\\n物种数量: {len(species_counts)}\")\n",
    "    print(f\"前10种物种:\")\n",
    "    for sp, cnt in species_counts.head(10).items():\n",
    "        print(f\"  {sp}: {cnt}\")\n",
    "    \n",
    "    # 选择Top物种进行可视化\n",
    "    top_n = min(15, len(species_counts))\n",
    "    top_species = species_counts.head(top_n).index.tolist()\n",
    "    \n",
    "    # 为每个物种分配颜色\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(top_species)))\n",
    "    species_colors = dict(zip(top_species, colors))\n",
    "    \n",
    "    # 为每条记录分配主要物种(取第一个入侵物种)\n",
    "    def get_primary_species(species_str):\n",
    "        if pd.isna(species_str):\n",
    "            return 'Other'\n",
    "        species_list = [s.strip() for s in str(species_str).split(',')]\n",
    "        for sp in species_list:\n",
    "            if sp in top_species:\n",
    "                return sp\n",
    "        return 'Other'\n",
    "    \n",
    "    df_plot['primary_species'] = df_plot['alien_species'].apply(get_primary_species)\n",
    "    \n",
    "    # 设置输出目录\n",
    "    if output_dir is None:\n",
    "        output_dir = Path(input_file).parent / 'visualizations'\n",
    "    else:\n",
    "        output_dir = Path(output_dir)\n",
    "    \n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    print(f\"\\n图片将保存到: {output_dir}\")\n",
    "    \n",
    "    # 加载中国地图\n",
    "    china_map = load_china_map()\n",
    "    \n",
    "    # 1. 总体分布图\n",
    "    print(\"\\n生成总体分布图...\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # 绘制地图底图\n",
    "    plot_china_basemap(ax, china_map)\n",
    "    \n",
    "    # 绘制物种分布点\n",
    "    for species in top_species:\n",
    "        species_data = df_plot[df_plot['primary_species'] == species]\n",
    "        ax.scatter(species_data['longitude'], species_data['latitude'],\n",
    "                  c=[species_colors[species]], label=species, \n",
    "                  alpha=0.6, s=50, edgecolors='white', linewidth=0.5)\n",
    "    \n",
    "    ax.set_xlabel('经度', fontsize=14)\n",
    "    ax.set_ylabel('纬度', fontsize=14)\n",
    "    # ax.set_title('入侵物种空间分布图', fontsize=18, fontweight='bold', pad=20)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    ax.grid(True, alpha=0.2, linestyle='--')\n",
    "    ax.set_xlim(73, 136)\n",
    "    ax.set_ylim(18, 54)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'overall_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ 已保存: overall_distribution.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # # 2. 按年份分布图\n",
    "    # print(\"\\n生成按年份分布图...\")\n",
    "    years = sorted(df_plot['year'].dropna().unique())\n",
    "    \n",
    "    # if len(years) > 0:\n",
    "    #     # 选择关键年份绘制\n",
    "    #     year_step = max(1, len(years) // 6)\n",
    "    #     key_years = years[::year_step]\n",
    "        \n",
    "    #     n_cols = 3\n",
    "    #     n_rows = (len(key_years) + n_cols - 1) // n_cols\n",
    "        \n",
    "    #     fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 5*n_rows))\n",
    "    #     axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "        \n",
    "    #     for idx, year in enumerate(key_years):\n",
    "    #         if idx >= len(axes):\n",
    "    #             break\n",
    "            \n",
    "    #         ax = axes[idx]\n",
    "            \n",
    "    #         # 绘制地图底图\n",
    "    #         plot_china_basemap(ax, china_map)\n",
    "            \n",
    "    #         year_data = df_plot[df_plot['year'] == year]\n",
    "            \n",
    "    #         for species in top_species:\n",
    "    #             species_data = year_data[year_data['primary_species'] == species]\n",
    "    #             if len(species_data) > 0:\n",
    "    #                 ax.scatter(species_data['longitude'], species_data['latitude'],\n",
    "    #                          c=[species_colors[species]], label=species, \n",
    "    #                          alpha=0.6, s=40, edgecolors='white', linewidth=0.5)\n",
    "            \n",
    "    #         ax.set_title(f'{year}年 (n={len(year_data)})', fontsize=14, fontweight='bold')\n",
    "    #         ax.set_xlabel('经度', fontsize=10)\n",
    "    #         ax.set_ylabel('纬度', fontsize=10)\n",
    "    #         ax.grid(True, alpha=0.2, linestyle='--')\n",
    "    #         ax.set_xlim(73, 136)\n",
    "    #         ax.set_ylim(18, 54)\n",
    "        \n",
    "    #     # 隐藏多余的子图\n",
    "    #     for idx in range(len(key_years), len(axes)):\n",
    "    #         axes[idx].set_visible(False)\n",
    "        \n",
    "    #     # 添加图例\n",
    "    #     handles, labels = axes[0].get_legend_handles_labels()\n",
    "    #     if handles:\n",
    "    #         fig.legend(handles, labels, loc='center', bbox_to_anchor=(0.5, -0.02), \n",
    "    #                   ncol=5, fontsize=10)\n",
    "        \n",
    "    #     plt.tight_layout()\n",
    "    #     plt.savefig(output_dir / 'distribution_by_year.png', dpi=300, bbox_inches='tight')\n",
    "    #     print(f\"✓ 已保存: distribution_by_year.png\")\n",
    "    #     plt.close()\n",
    "    \n",
    "    # 3. 时间序列累积图\n",
    "    print(\"\\n生成时间序列累积图...\")\n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "    \n",
    "    time_species_counts = []\n",
    "    for year in sorted(years):\n",
    "        year_data = df_plot[df_plot['year'] <= year]\n",
    "        species_dist = year_data['primary_species'].value_counts()\n",
    "        time_species_counts.append({'year': year, **species_dist.to_dict()})\n",
    "    \n",
    "    time_df = pd.DataFrame(time_species_counts).fillna(0)\n",
    "    \n",
    "    # 绘制堆叠面积图\n",
    "    time_df_sorted = time_df.set_index('year')\n",
    "    bottom = np.zeros(len(time_df_sorted))\n",
    "    \n",
    "    for species in top_species:\n",
    "        if species in time_df_sorted.columns:\n",
    "            values = time_df_sorted[species].values\n",
    "            ax.fill_between(time_df_sorted.index, bottom, bottom + values, \n",
    "                           label=species, alpha=0.7, color=species_colors[species])\n",
    "            bottom += values\n",
    "    \n",
    "    ax.set_xlabel('年份', fontsize=14)\n",
    "    ax.set_ylabel('累积记录数', fontsize=14)\n",
    "    # ax.set_title('入侵物种累积分布趋势', fontsize=18, fontweight='bold', pad=20)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # 旋转x轴标签并调整间隔\n",
    "    years_list = sorted(time_df_sorted.index.astype(str))\n",
    "    if len(years_list) > 20:\n",
    "        # 如果年份太多，只显示部分标签\n",
    "        step = len(years_list) // 15\n",
    "        ax.set_xticks(range(0, len(years_list), step))\n",
    "        ax.set_xticklabels([years_list[i] for i in range(0, len(years_list), step)], \n",
    "                          rotation=45, ha='right')\n",
    "    else:\n",
    "        ax.set_xticklabels(years_list, rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'cumulative_trend.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ 已保存: cumulative_trend.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. 热力图 - 物种×年份\n",
    "    print(\"\\n生成物种-年份热力图...\")\n",
    "    pivot_data = df_plot.groupby(['year', 'primary_species']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # 只保留Top物种\n",
    "    pivot_cols = [col for col in pivot_data.columns if col in top_species]\n",
    "    pivot_data = pivot_data[pivot_cols]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "    sns.heatmap(pivot_data.T, cmap='YlOrRd', annot=False, fmt='d', \n",
    "                cbar_kws={'label': '记录数'}, ax=ax, linewidths=0.5)\n",
    "    ax.set_xlabel('年份', fontsize=14)\n",
    "    ax.set_ylabel('物种', fontsize=14)\n",
    "    # ax.set_title('入侵物种时间分布热力图', fontsize=18, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'species_year_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ 已保存: species_year_heatmap.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Top物种的地理分布对比\n",
    "    print(\"\\n生成Top物种地理分布对比图...\")\n",
    "    top_5_species = species_counts.head(5).index.tolist()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(24, 13))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, species in enumerate(top_5_species):\n",
    "        if idx >= len(axes):\n",
    "            break\n",
    "        \n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # 绘制地图底图\n",
    "        plot_china_basemap(ax, china_map)\n",
    "        \n",
    "        species_data = df_plot[df_plot['primary_species'] == species]\n",
    "        \n",
    "        ax.scatter(species_data['longitude'], species_data['latitude'],\n",
    "                  c=[species_colors[species]], alpha=0.6, s=50, \n",
    "                  edgecolors='white', linewidth=0.5)\n",
    "        \n",
    "        ax.set_title(f'{species}\\n(n={len(species_data)})', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('经度', fontsize=10)\n",
    "        ax.set_ylabel('纬度', fontsize=10)\n",
    "        ax.grid(True, alpha=0.2, linestyle='--')\n",
    "        ax.set_xlim(73, 136)\n",
    "        ax.set_ylim(18, 54)\n",
    "    \n",
    "    # 最后一个子图显示所有物种\n",
    "    ax = axes[5]\n",
    "    \n",
    "    # 绘制地图底图\n",
    "    plot_china_basemap(ax, china_map)\n",
    "    \n",
    "    for species in top_5_species:\n",
    "        species_data = df_plot[df_plot['primary_species'] == species]\n",
    "        ax.scatter(species_data['longitude'], species_data['latitude'],\n",
    "                  c=[species_colors[species]], label=species, \n",
    "                  alpha=0.5, s=30, edgecolors='white', linewidth=0.3)\n",
    "    \n",
    "    ax.set_title('所有Top5物种分布', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('经度', fontsize=10)\n",
    "    ax.set_ylabel('纬度', fontsize=10)\n",
    "    ax.legend(fontsize=8, loc='best')\n",
    "    ax.grid(True, alpha=0.2, linestyle='--')\n",
    "    ax.set_xlim(73, 136)\n",
    "    ax.set_ylim(18, 54)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'top5_species_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ 已保存: top5_species_comparison.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. 密度分布图 (额外添加)\n",
    "    print(\"\\n生成密度分布热力图...\")\n",
    "    fig, ax = plt.subplots(figsize=(16, 12))\n",
    "    \n",
    "    # 绘制地图底图\n",
    "    plot_china_basemap(ax, china_map)\n",
    "    \n",
    "    # 绘制六边形密度图\n",
    "    hb = ax.hexbin(df_plot['longitude'], df_plot['latitude'], \n",
    "                   gridsize=30, cmap='YlOrRd', alpha=0.7, \n",
    "                   mincnt=1, edgecolors='white', linewidths=0.2)\n",
    "    \n",
    "    cb = plt.colorbar(hb, ax=ax)\n",
    "    cb.set_label('记录密度', fontsize=12)\n",
    "    \n",
    "    ax.set_xlabel('经度 (Longitude)', fontsize=14)\n",
    "    ax.set_ylabel('纬度 (Latitude)', fontsize=14)\n",
    "    ax.set_title('入侵物种密度分布图', fontsize=18, fontweight='bold', pad=20)\n",
    "    ax.grid(True, alpha=0.2, linestyle='--')\n",
    "    ax.set_xlim(73, 136)\n",
    "    ax.set_ylim(18, 54)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'density_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ 已保存: density_distribution.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 生成总结\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"可视化完成\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"总记录数: {len(df_plot)}\")\n",
    "    print(f\"时间范围: {years[0]} - {years[-1]}\")\n",
    "    print(f\"可视化物种数: {len(top_species)}\")\n",
    "    print(f\"输出目录: {output_dir}\")\n",
    "    print(f\"\\n生成的图片:\")\n",
    "    print(f\"  1. overall_distribution.png - 总体空间分布\")\n",
    "    print(f\"  2. distribution_by_year.png - 按年份分布\")\n",
    "    print(f\"  3. cumulative_trend.png - 累积趋势\")\n",
    "    print(f\"  4. species_year_heatmap.png - 时间热力图\")\n",
    "    print(f\"  5. top5_species_comparison.png - Top5物种对比\")\n",
    "    print(f\"  6. density_distribution.png - 密度分布图 (新增)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return df_plot\n",
    "\n",
    "\n",
    "# 主函数\n",
    "if __name__ == \"__main__\":\n",
    "    # 执行可视化\n",
    "    df_plot = visualize_species_distribution(\n",
    "        'output/backups/backup_final_04334_retry_final.xlsx'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
